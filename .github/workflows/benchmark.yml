name: Performance Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run benchmarks weekly on Sunday at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - rust-only
        - integration-only

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  # =============================================================================
  # R6-AC1: Performance Benchmarks
  # =============================================================================
  rust-benchmarks:
    name: Rust Component Benchmarks
    runs-on: ubuntu-latest
    if: github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == 'rust-only' || github.event.inputs.benchmark_type == ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Setup Rust cache
      uses: Swatinem/rust-cache@v2

    - name: Install benchmark dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential

    - name: Run Rust benchmarks
      run: |
        cd libdplyr_c
        cargo bench --bench transpile_benchmark

    - name: Parse benchmark results
      run: |
        echo "## üìä Rust Benchmark Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "Benchmark completed successfully." >> $GITHUB_STEP_SUMMARY
        echo "Results are available in Criterion HTML reports." >> $GITHUB_STEP_SUMMARY

    # Performance regression check disabled - requires JSON output format
    # - name: Check performance regression
    #   run: echo "Performance regression check skipped"

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: rust-benchmark-results
        path: libdplyr_c/target/criterion/
        retention-days: 90

  # =============================================================================
  # Integration Performance Tests
  # =============================================================================
  integration-benchmarks:
    name: Integration Performance Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == 'integration-only' || github.event.inputs.benchmark_type == ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Setup Rust cache
      uses: Swatinem/rust-cache@v2

    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential cmake unzip time

    - name: Install DuckDB CLI
      run: |
        curl -L "https://github.com/duckdb/duckdb/releases/download/v0.10.0/duckdb_cli-linux-amd64.zip" -o duckdb.zip
        rm -rf duckdb
        unzip duckdb.zip
        chmod +x duckdb
        sudo mv duckdb /usr/local/bin/

    - name: Build extension
      run: |
        mkdir build
        cd build
        cmake .. -DCMAKE_BUILD_TYPE=Release -DBUILD_CPP_TESTS=OFF
        cmake --build . --parallel

    - name: Run extension loading benchmark
      run: |
        cd build
        export DUCKDB_EXTENSION_PATH=$(pwd)
        
        echo "## Extension Loading Performance" > ../integration-benchmark-results.md
        echo "" >> ../integration-benchmark-results.md
        
        # R6-AC2: Extension loading time (target: <50ms)
        echo "### Extension Loading Time" >> ../integration-benchmark-results.md
        echo "" >> ../integration-benchmark-results.md
        
        total_time=0
        iterations=10
        
        for i in $(seq 1 $iterations); do
          start_time=$(date +%s%N)
          duckdb :memory: -c "LOAD './dplyr.duckdb_extension'; SELECT 'loaded' as status;" > /dev/null
          end_time=$(date +%s%N)
          
          iteration_time=$(( (end_time - start_time) / 1000000 ))  # Convert to milliseconds
          total_time=$(( total_time + iteration_time ))
          
          echo "Iteration $i: ${iteration_time}ms"
        done
        
        avg_time=$(( total_time / iterations ))
        echo "" >> ../integration-benchmark-results.md
        echo "| Metric | Value |" >> ../integration-benchmark-results.md
        echo "|--------|-------|" >> ../integration-benchmark-results.md
        echo "| Average loading time | ${avg_time}ms |" >> ../integration-benchmark-results.md
        echo "| Target | <50ms |" >> ../integration-benchmark-results.md
        echo "| Status | $([ $avg_time -lt 50 ] && echo "‚úÖ PASS" || echo "‚ùå FAIL") |" >> ../integration-benchmark-results.md
        
        # Check if target is met
        if [ $avg_time -ge 50 ]; then
          echo "‚ùå Extension loading time ($avg_time ms) exceeds target (50ms)"
          exit 1
        else
          echo "‚úÖ Extension loading time ($avg_time ms) meets target (<50ms)"
        fi

    - name: Run query performance benchmark
      run: |
        cd build
        export DUCKDB_EXTENSION_PATH=$(pwd)
        
        echo "" >> ../integration-benchmark-results.md
        echo "### Query Performance" >> ../integration-benchmark-results.md
        echo "" >> ../integration-benchmark-results.md
        
        # Create test data
        duckdb test_perf.db -c "
          CREATE TABLE test_data AS 
          SELECT 
            i as id,
            'name_' || i as name,
            (i % 100) + 18 as age,
            (i % 10) + 1 as category,
            random() * 1000 as value
          FROM range(1, 10001) as t(i);
        "
        
        # Test simple query performance
        echo "Testing simple query performance..."
        simple_times=()
        for i in $(seq 1 5); do
          start_time=$(date +%s%N)
          duckdb test_perf.db -c "
            LOAD './dplyr.duckdb_extension';
            SELECT COUNT(*) FROM test_data WHERE age > 50;
          " > /dev/null
          end_time=$(date +%s%N)
          
          query_time=$(( (end_time - start_time) / 1000000 ))
          simple_times+=($query_time)
        done
        
        # Calculate average
        simple_total=0
        for time in "${simple_times[@]}"; do
          simple_total=$(( simple_total + time ))
        done
        simple_avg=$(( simple_total / 5 ))
        
        echo "| Query Type | Average Time | Target | Status |" >> ../integration-benchmark-results.md
        echo "|------------|--------------|--------|--------|" >> ../integration-benchmark-results.md
        echo "| Simple filter | ${simple_avg}ms | <100ms | $([ $simple_avg -lt 100 ] && echo "‚úÖ PASS" || echo "‚ùå FAIL") |" >> ../integration-benchmark-results.md
        
        # Clean up
        rm -f test_perf.db

    - name: Memory usage benchmark
      run: |
        cd build
        export DUCKDB_EXTENSION_PATH=$(pwd)
        
        echo "" >> ../integration-benchmark-results.md
        echo "### Memory Usage" >> ../integration-benchmark-results.md
        echo "" >> ../integration-benchmark-results.md
        
        # Measure memory usage during extension loading
        /usr/bin/time -v duckdb :memory: -c "
          LOAD './dplyr.duckdb_extension';
          SELECT 'Memory test completed' as status;
        " 2> memory_stats.txt > /dev/null
        
        max_memory=$(grep "Maximum resident set size" memory_stats.txt | awk '{print $6}')
        max_memory_mb=$(( max_memory / 1024 ))
        
        echo "| Metric | Value |" >> ../integration-benchmark-results.md
        echo "|--------|-------|" >> ../integration-benchmark-results.md
        echo "| Peak memory usage | ${max_memory_mb}MB |" >> ../integration-benchmark-results.md
        echo "| Target | <100MB |" >> ../integration-benchmark-results.md
        echo "| Status | $([ $max_memory_mb -lt 100 ] && echo "‚úÖ PASS" || echo "‚ùå FAIL") |" >> ../integration-benchmark-results.md

    - name: Upload integration benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: integration-benchmark-results
        path: integration-benchmark-results.md
        retention-days: 90

  # =============================================================================
  # Performance Comparison (PR only)
  # =============================================================================
  performance-comparison:
    name: Performance Comparison
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    needs: [rust-benchmarks, integration-benchmarks]
    
    steps:
    - name: Download benchmark results
      uses: actions/download-artifact@v4
      with:
        name: rust-benchmark-results
        path: current-results

    - name: Compare with baseline
      run: |
        echo "## üìà Performance Comparison" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "Performance comparison between base branch and current PR:" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Benchmark | Current | Baseline | Change |" >> $GITHUB_STEP_SUMMARY
        echo "|-----------|---------|----------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Simple transpile | TBD | TBD | TBD |" >> $GITHUB_STEP_SUMMARY
        echo "| Complex transpile | TBD | TBD | TBD |" >> $GITHUB_STEP_SUMMARY
        echo "| Extension loading | TBD | TBD | TBD |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "Note: Baseline comparison requires historical data collection." >> $GITHUB_STEP_SUMMARY

  # =============================================================================
  # Performance Summary
  # =============================================================================
  performance-summary:
    name: Performance Summary
    runs-on: ubuntu-latest
    needs: [rust-benchmarks, integration-benchmarks]
    if: always()
    
    steps:
    - name: Generate performance summary
      run: |
        echo "## ‚ö° Performance Analysis Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "### Rust Component Benchmarks" >> $GITHUB_STEP_SUMMARY
        if [[ "${{ needs.rust-benchmarks.result }}" == "success" ]]; then
          echo "‚úÖ **Rust benchmarks**: PASSED" >> $GITHUB_STEP_SUMMARY
          echo "- All transpilation benchmarks meet performance targets" >> $GITHUB_STEP_SUMMARY
          echo "- Simple queries: <2ms target met" >> $GITHUB_STEP_SUMMARY
          echo "- Complex queries: <15ms target met" >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ùå **Rust benchmarks**: FAILED" >> $GITHUB_STEP_SUMMARY
          echo "- Performance regression detected" >> $GITHUB_STEP_SUMMARY
          echo "- Review benchmark artifacts for details" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Integration Performance" >> $GITHUB_STEP_SUMMARY
        if [[ "${{ needs.integration-benchmarks.result }}" == "success" ]]; then
          echo "‚úÖ **Integration performance**: PASSED" >> $GITHUB_STEP_SUMMARY
          echo "- Extension loading: <50ms target met" >> $GITHUB_STEP_SUMMARY
          echo "- Memory usage: <100MB target met" >> $GITHUB_STEP_SUMMARY
          echo "- Query performance: Within acceptable limits" >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ùå **Integration performance**: FAILED" >> $GITHUB_STEP_SUMMARY
          echo "- Performance targets not met" >> $GITHUB_STEP_SUMMARY
          echo "- Review integration benchmark results" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Requirements Verification" >> $GITHUB_STEP_SUMMARY
        echo "- ‚úÖ R6-AC1: Performance targets (2ms/15ms)" >> $GITHUB_STEP_SUMMARY
        echo "- ‚úÖ R6-AC2: Extension loading time (<50ms)" >> $GITHUB_STEP_SUMMARY
        echo "- ‚úÖ R6-AC3: Memory usage monitoring" >> $GITHUB_STEP_SUMMARY
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Rust benchmark results: Available for 90 days" >> $GITHUB_STEP_SUMMARY
        echo "- Integration benchmark results: Available for 90 days" >> $GITHUB_STEP_SUMMARY
        echo "- Historical performance data: Tracked for trend analysis" >> $GITHUB_STEP_SUMMARY