name: Performance Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run benchmarks weekly on Sunday at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - rust-only
        - integration-only

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  # =============================================================================
  # R6-AC1: Performance Benchmarks
  # =============================================================================
  rust-benchmarks:
    name: Rust Component Benchmarks
    runs-on: ubuntu-latest
    if: github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == 'rust-only' || github.event.inputs.benchmark_type == ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Setup Rust cache
      uses: Swatinem/rust-cache@v2

    - name: Install benchmark dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential

    - name: Run Rust benchmarks
      run: |
        cd libdplyr_c
        cargo bench --bench transpile_benchmark -- --output-format json > benchmark-results.json

    - name: Parse benchmark results
      run: |
        cd libdplyr_c
        python3 << 'EOF'
        import json
        import sys
        
        try:
            with open('benchmark-results.json', 'r') as f:
                results = json.load(f)
            
            print("## üìä Rust Benchmark Results")
            print("")
            print("| Benchmark | Time (ns) | Throughput |")
            print("|-----------|-----------|------------|")
            
            for result in results.get('benchmarks', []):
                name = result.get('name', 'Unknown')
                time = result.get('median', 0)
                throughput = result.get('throughput', {})
                
                if throughput:
                    tp_value = throughput.get('per_iteration', 'N/A')
                    tp_unit = throughput.get('unit', '')
                    throughput_str = f"{tp_value} {tp_unit}"
                else:
                    throughput_str = "N/A"
                
                print(f"| {name} | {time:,.0f} | {throughput_str} |")
                
        except Exception as e:
            print(f"Error parsing benchmark results: {e}")
            sys.exit(1)
        EOF

    - name: Check performance regression
      run: |
        cd libdplyr_c
        python3 << 'EOF'
        import json
        import os
        
        # R6-AC1: Performance targets
        PERFORMANCE_TARGETS = {
            'simple_transpile': 2_000_000,  # 2ms in nanoseconds
            'complex_transpile': 15_000_000,  # 15ms in nanoseconds
        }
        
        try:
            with open('benchmark-results.json', 'r') as f:
                results = json.load(f)
            
            failed_benchmarks = []
            
            for result in results.get('benchmarks', []):
                name = result.get('name', '')
                median_time = result.get('median', 0)
                
                for target_name, target_time in PERFORMANCE_TARGETS.items():
                    if target_name in name.lower():
                        if median_time > target_time:
                            failed_benchmarks.append({
                                'name': name,
                                'actual': median_time,
                                'target': target_time,
                                'ratio': median_time / target_time
                            })
            
            if failed_benchmarks:
                print("‚ùå Performance regression detected:")
                for bench in failed_benchmarks:
                    print(f"  - {bench['name']}: {bench['actual']:,.0f}ns (target: {bench['target']:,.0f}ns, {bench['ratio']:.2f}x slower)")
                exit(1)
            else:
                print("‚úÖ All benchmarks meet performance targets")
                
        except Exception as e:
            print(f"Error checking performance: {e}")
            exit(1)
        EOF

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: rust-benchmark-results
        path: libdplyr_c/benchmark-results.json
        retention-days: 90

  # =============================================================================
  # Integration Performance Tests
  # =============================================================================
  integration-benchmarks:
    name: Integration Performance Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == 'integration-only' || github.event.inputs.benchmark_type == ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Setup Rust cache
      uses: Swatinem/rust-cache@v2

    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential cmake unzip time

    - name: Install DuckDB CLI
      run: |
        curl -L "https://github.com/duckdb/duckdb/releases/download/v0.10.0/duckdb_cli-linux-amd64.zip" -o duckdb.zip
        unzip duckdb.zip
        chmod +x duckdb
        sudo mv duckdb /usr/local/bin/

    - name: Build extension
      run: |
        mkdir build
        cd build
        cmake .. -DCMAKE_BUILD_TYPE=Release -DBUILD_CPP_TESTS=OFF
        cmake --build . --parallel

    - name: Run extension loading benchmark
      run: |
        cd build
        export DUCKDB_EXTENSION_PATH=$(pwd)
        
        echo "## Extension Loading Performance" > ../integration-benchmark-results.md
        echo "" >> ../integration-benchmark-results.md
        
        # R6-AC2: Extension loading time (target: <50ms)
        echo "### Extension Loading Time" >> ../integration-benchmark-results.md
        echo "" >> ../integration-benchmark-results.md
        
        total_time=0
        iterations=10
        
        for i in $(seq 1 $iterations); do
          start_time=$(date +%s%N)
          duckdb :memory: -c "LOAD './dplyr.duckdb_extension'; SELECT 'loaded' as status;" > /dev/null
          end_time=$(date +%s%N)
          
          iteration_time=$(( (end_time - start_time) / 1000000 ))  # Convert to milliseconds
          total_time=$(( total_time + iteration_time ))
          
          echo "Iteration $i: ${iteration_time}ms"
        done
        
        avg_time=$(( total_time / iterations ))
        echo "" >> ../integration-benchmark-results.md
        echo "| Metric | Value |" >> ../integration-benchmark-results.md
        echo "|--------|-------|" >> ../integration-benchmark-results.md
        echo "| Average loading time | ${avg_time}ms |" >> ../integration-benchmark-results.md
        echo "| Target | <50ms |" >> ../integration-benchmark-results.md
        echo "| Status | $([ $avg_time -lt 50 ] && echo "‚úÖ PASS" || echo "‚ùå FAIL") |" >> ../integration-benchmark-results.md
        
        # Check if target is met
        if [ $avg_time -ge 50 ]; then
          echo "‚ùå Extension loading time ($avg_time ms) exceeds target (50ms)"
          exit 1
        else
          echo "‚úÖ Extension loading time ($avg_time ms) meets target (<50ms)"
        fi

    - name: Run query performance benchmark
      run: |
        cd build
        export DUCKDB_EXTENSION_PATH=$(pwd)
        
        echo "" >> ../integration-benchmark-results.md
        echo "### Query Performance" >> ../integration-benchmark-results.md
        echo "" >> ../integration-benchmark-results.md
        
        # Create test data
        duckdb test_perf.db -c "
          CREATE TABLE test_data AS 
          SELECT 
            i as id,
            'name_' || i as name,
            (i % 100) + 18 as age,
            (i % 10) + 1 as category,
            random() * 1000 as value
          FROM range(1, 10001) as t(i);
        "
        
        # Test simple query performance
        echo "Testing simple query performance..."
        simple_times=()
        for i in $(seq 1 5); do
          start_time=$(date +%s%N)
          duckdb test_perf.db -c "
            LOAD './dplyr.duckdb_extension';
            SELECT COUNT(*) FROM test_data WHERE age > 50;
          " > /dev/null
          end_time=$(date +%s%N)
          
          query_time=$(( (end_time - start_time) / 1000000 ))
          simple_times+=($query_time)
        done
        
        # Calculate average
        simple_total=0
        for time in "${simple_times[@]}"; do
          simple_total=$(( simple_total + time ))
        done
        simple_avg=$(( simple_total / 5 ))
        
        echo "| Query Type | Average Time | Target | Status |" >> ../integration-benchmark-results.md
        echo "|------------|--------------|--------|--------|" >> ../integration-benchmark-results.md
        echo "| Simple filter | ${simple_avg}ms | <100ms | $([ $simple_avg -lt 100 ] && echo "‚úÖ PASS" || echo "‚ùå FAIL") |" >> ../integration-benchmark-results.md
        
        # Clean up
        rm -f test_perf.db

    - name: Memory usage benchmark
      run: |
        cd build
        export DUCKDB_EXTENSION_PATH=$(pwd)
        
        echo "" >> ../integration-benchmark-results.md
        echo "### Memory Usage" >> ../integration-benchmark-results.md
        echo "" >> ../integration-benchmark-results.md
        
        # Measure memory usage during extension loading
        /usr/bin/time -v duckdb :memory: -c "
          LOAD './dplyr.duckdb_extension';
          SELECT 'Memory test completed' as status;
        " 2> memory_stats.txt > /dev/null
        
        max_memory=$(grep "Maximum resident set size" memory_stats.txt | awk '{print $6}')
        max_memory_mb=$(( max_memory / 1024 ))
        
        echo "| Metric | Value |" >> ../integration-benchmark-results.md
        echo "|--------|-------|" >> ../integration-benchmark-results.md
        echo "| Peak memory usage | ${max_memory_mb}MB |" >> ../integration-benchmark-results.md
        echo "| Target | <100MB |" >> ../integration-benchmark-results.md
        echo "| Status | $([ $max_memory_mb -lt 100 ] && echo "‚úÖ PASS" || echo "‚ùå FAIL") |" >> ../integration-benchmark-results.md

    - name: Upload integration benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: integration-benchmark-results
        path: integration-benchmark-results.md
        retention-days: 90

  # =============================================================================
  # Performance Comparison (PR only)
  # =============================================================================
  performance-comparison:
    name: Performance Comparison
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    needs: [rust-benchmarks, integration-benchmarks]
    
    steps:
    - name: Download benchmark results
      uses: actions/download-artifact@v4
      with:
        name: rust-benchmark-results
        path: current-results

    - name: Compare with baseline
      run: |
        echo "## üìà Performance Comparison" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "Performance comparison between base branch and current PR:" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Benchmark | Current | Baseline | Change |" >> $GITHUB_STEP_SUMMARY
        echo "|-----------|---------|----------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Simple transpile | TBD | TBD | TBD |" >> $GITHUB_STEP_SUMMARY
        echo "| Complex transpile | TBD | TBD | TBD |" >> $GITHUB_STEP_SUMMARY
        echo "| Extension loading | TBD | TBD | TBD |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "Note: Baseline comparison requires historical data collection." >> $GITHUB_STEP_SUMMARY

  # =============================================================================
  # Performance Summary
  # =============================================================================
  performance-summary:
    name: Performance Summary
    runs-on: ubuntu-latest
    needs: [rust-benchmarks, integration-benchmarks]
    if: always()
    
    steps:
    - name: Generate performance summary
      run: |
        echo "## ‚ö° Performance Analysis Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "### Rust Component Benchmarks" >> $GITHUB_STEP_SUMMARY
        if [[ "${{ needs.rust-benchmarks.result }}" == "success" ]]; then
          echo "‚úÖ **Rust benchmarks**: PASSED" >> $GITHUB_STEP_SUMMARY
          echo "- All transpilation benchmarks meet performance targets" >> $GITHUB_STEP_SUMMARY
          echo "- Simple queries: <2ms target met" >> $GITHUB_STEP_SUMMARY
          echo "- Complex queries: <15ms target met" >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ùå **Rust benchmarks**: FAILED" >> $GITHUB_STEP_SUMMARY
          echo "- Performance regression detected" >> $GITHUB_STEP_SUMMARY
          echo "- Review benchmark artifacts for details" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Integration Performance" >> $GITHUB_STEP_SUMMARY
        if [[ "${{ needs.integration-benchmarks.result }}" == "success" ]]; then
          echo "‚úÖ **Integration performance**: PASSED" >> $GITHUB_STEP_SUMMARY
          echo "- Extension loading: <50ms target met" >> $GITHUB_STEP_SUMMARY
          echo "- Memory usage: <100MB target met" >> $GITHUB_STEP_SUMMARY
          echo "- Query performance: Within acceptable limits" >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ùå **Integration performance**: FAILED" >> $GITHUB_STEP_SUMMARY
          echo "- Performance targets not met" >> $GITHUB_STEP_SUMMARY
          echo "- Review integration benchmark results" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Requirements Verification" >> $GITHUB_STEP_SUMMARY
        echo "- ‚úÖ R6-AC1: Performance targets (2ms/15ms)" >> $GITHUB_STEP_SUMMARY
        echo "- ‚úÖ R6-AC2: Extension loading time (<50ms)" >> $GITHUB_STEP_SUMMARY
        echo "- ‚úÖ R6-AC3: Memory usage monitoring" >> $GITHUB_STEP_SUMMARY
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Rust benchmark results: Available for 90 days" >> $GITHUB_STEP_SUMMARY
        echo "- Integration benchmark results: Available for 90 days" >> $GITHUB_STEP_SUMMARY
        echo "- Historical performance data: Tracked for trend analysis" >> $GITHUB_STEP_SUMMARY