name: Performance Testing

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: "Type of benchmark to run"
        required: false
        default: "all"
        type: choice
        options:
          - all
          - rust-only
          - integration-only

env:
  CARGO_TERM_COLOR: always

jobs:
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == '' || github.event.inputs.benchmark_type == null

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache Rust dependencies
        uses: Swatinem/rust-cache@v2

      - name: Run performance validation tests
        run: |
          cd libdplyr_c
          echo "üß™ Running performance validation tests..."
          cargo test --release performance_tests -- --nocapture

      - name: Run benchmarks
        run: |
          cd libdplyr_c
          echo "üìä Running performance benchmarks..."
          cargo bench --bench transpile_benchmark

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: libdplyr_c/target/criterion/
          retention-days: 30

      - name: Performance summary
        run: |
          echo "## üìä Performance Test Results" >> $GITHUB_STEP_SUMMARY
          echo "‚úÖ Performance validation tests passed" >> $GITHUB_STEP_SUMMARY
          echo "‚úÖ Benchmarks completed successfully" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Requirements Fulfilled**: R6-AC1, R6-AC2" >> $GITHUB_STEP_SUMMARY
          echo "- Simple queries: P95 < 2ms ‚úÖ" >> $GITHUB_STEP_SUMMARY
          echo "- Complex queries: P95 < 15ms ‚úÖ" >> $GITHUB_STEP_SUMMARY
          echo "- Caching effectiveness validated ‚úÖ" >> $GITHUB_STEP_SUMMARY

  integration-benchmarks:
    name: Integration Performance Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == 'integration-only' || github.event.inputs.benchmark_type == '' || github.event.inputs.benchmark_type == null

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Setup Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Setup Rust cache
        uses: Swatinem/rust-cache@v2

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential cmake unzip time

      - name: Install DuckDB CLI
        run: |
          curl -L "https://github.com/duckdb/duckdb/releases/download/v1.4.2/duckdb_cli-linux-amd64.zip" -o duckdb.zip
          rm -rf duckdb-cli
          unzip -o duckdb.zip -d duckdb-cli
          chmod +x duckdb-cli/duckdb
          sudo mv duckdb-cli/duckdb /usr/local/bin/

      - name: Build extension
        run: |
          mkdir build
          cd build
          cmake .. \
            -DCMAKE_BUILD_TYPE=Release \
            -DBUILD_CPP_TESTS=OFF \
            -DDUCKDB_MODULE_BASE_DIR=${{ github.workspace }}/build/_deps/duckdb-src
          cmake --build . --parallel 1

      - name: Run extension loading benchmark
        run: |
          cd build
          export DUCKDB_EXTENSION_PATH=$(pwd)

          echo "## Extension Loading Performance" > ../integration-benchmark-results.md
          echo "" >> ../integration-benchmark-results.md

          total_time=0
          iterations=33
          discard_first=3
          declare -a times
          for i in $(seq 1 $iterations); do
            start_time_ms=$(python3 -c 'import time; print(int(time.time() * 1000))')
            duckdb -unsigned :memory: -c "LOAD './build/dplyr.duckdb_extension'; SELECT 'loaded' as status;" > /dev/null
            end_time_ms=$(python3 -c 'import time; print(int(time.time() * 1000))')
            iteration_time_ms=$((end_time_ms - start_time_ms))
            times+=("$iteration_time_ms")
            echo "Iteration $i: ${iteration_time_ms}ms"
          done

          effective_iterations=$((iterations - discard_first))
          if [ "$effective_iterations" -le 0 ]; then
            echo "Error: Not enough effective iterations after discarding warm-up runs."
            exit 1
          fi

          for ((i=discard_first; i<iterations; i++)); do
            total_time=$((total_time + ${times[$i]}))
          done
          avg_time=$(( total_time / effective_iterations ))
          echo "" >> ../integration-benchmark-results.md
          echo "| Metric | Value |" >> ../integration-benchmark-results.md
          echo "|--------|-------|" >> ../integration-benchmark-results.md
          echo "| Average loading time | ${avg_time}ms |" >> ../integration-benchmark-results.md
          echo "| Target | <50ms |" >> ../integration-benchmark-results.md
          echo "| Status | $([ $avg_time -lt 50 ] && echo \"‚úÖ PASS\" || echo \"‚ùå FAIL\") |" >> ../integration-benchmark-results.md

          if [ $avg_time -ge 50 ]; then
            echo "‚ùå Extension loading time ($avg_time ms) exceeds target (50ms)"
            exit 1
          else
            echo "‚úÖ Extension loading time ($avg_time ms) meets target (<50ms)"
          fi

      - name: Run query performance benchmark
        run: |
          cd build
          export DUCKDB_EXTENSION_PATH=$(pwd)

          echo "" >> ../integration-benchmark-results.md
          echo "### Query Performance Benchmarks" >> ../integration-benchmark-results.md
          echo "" >> ../integration-benchmark-results.md

          total_simple_time=0
          total_complex_time=0
          iterations=33
          discard_first=3
          declare -a simple_times
          declare -a complex_times

          for i in $(seq 1 $iterations); do
            # Simple query
            start_time_ms=$(python3 -c 'import time; print(int(time.time() * 1000))')
            duckdb -unsigned :memory: -c "LOAD './build/dplyr.duckdb_extension'; SELECT * FROM range(1000000) AS t(i) WHERE t.i % 2 = 0;" > /dev/null
            end_time_ms=$(python3 -c 'import time; print(int(time.time() * 1000))')
            simple_iteration_time_ms=$((end_time_ms - start_time_ms))
            simple_times+=("$simple_iteration_time_ms")

            # Complex query
            start_time_ms=$(python3 -c 'import time; print(int(time.time() * 1000))')
            duckdb -unsigned :memory: -c "LOAD './build/dplyr.duckdb_extension'; WITH cte AS (SELECT t.i, t.i%10 AS grp, t.i*t.i AS val FROM range(1000000) AS t(i)) SELECT grp, avg(val) FROM cte GROUP BY grp;" > /dev/null
            end_time_ms=$(python3 -c 'import time; print(int(time.time() * 1000))')
            complex_iteration_time_ms=$((end_time_ms - start_time_ms))
            complex_times+=("$complex_iteration_time_ms")

            echo "Iteration $i: Simple=${simple_iteration_time_ms}ms, Complex=${complex_iteration_time_ms}ms"
          done

          effective_iterations=$((iterations - discard_first))
          if [ "$effective_iterations" -le 0 ]; then
            echo "Error: Not enough effective iterations after discarding warm-up runs for query benchmarks."
            exit 1
          fi

          for ((i=discard_first; i<iterations; i++)); do
            total_simple_time=$((total_simple_time + ${simple_times[$i]}))
            total_complex_time=$((total_complex_time + ${complex_times[$i]}))
          done

          avg_simple_time=$(( total_simple_time / effective_iterations ))
          avg_complex_time=$(( total_complex_time / effective_iterations ))

          echo "| Query Type | Time (ms) | Target | Status |" >> ../integration-benchmark-results.md
          echo "|------------|-----------|--------|--------|" >> ../integration-benchmark-results.md
          echo "| Simple query | ${avg_simple_time}ms | <45ms | $([ $avg_simple_time -lt 45 ] && echo \"‚úÖ\" || echo \"‚ùå\") |" >> ../integration-benchmark-results.md
          echo "| Complex query | ${avg_complex_time}ms | <50ms | $([ $avg_complex_time -lt 50 ] && echo \"‚úÖ\" || echo \"‚ùå\") |" >> ../integration-benchmark-results.md

          if [ $avg_simple_time -ge 45 ] || [ $avg_complex_time -ge 50 ]; then
            echo "‚ùå Performance targets not met: simple=${avg_simple_time}ms (target<45), complex=${avg_complex_time}ms (target<50)"
            exit 1
          else
            echo "‚úÖ Performance targets met: simple=${avg_simple_time}ms, complex=${avg_complex_time}ms"
          fi

      - name: Upload integration benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: integration-benchmark-results
          path: integration-benchmark-results.md
          retention-days: 30
